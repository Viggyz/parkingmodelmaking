{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "effnetliteipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTLGmtqp8E-Y"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kksXN9ba_q6"
      },
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "import os\n",
        "import pathlib\n",
        "import PIL\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aZIinn9bDS6"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "train_zip = 'drive/MyDrive/Datasets/traindata.zip'\n",
        "val_zip = 'drive/MyDrive/Datasets/val_data.zip'\n",
        "test_zip = 'drive/MyDrive/Datasets/test_data.zip'\n",
        "\n",
        "with ZipFile(test_zip, 'r') as zip:\n",
        "  zip.extractall(\"test_photos\")\n",
        "  \n",
        "# train_dir = os.path.join(\"./\", 'train_photos')\n",
        "# val_dir = os.path.join(\"./\", 'val_photos')\n",
        "test_dir = os.path.join(\"./\", 'test_photos')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRS0AcIiYMpG"
      },
      "source": [
        "data_dir = pathlib.Path(test_dir)\n",
        "free = list(data_dir.glob('free/*'))\n",
        "PIL.Image.open(str(free[5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW_abhI4bMfP"
      },
      "source": [
        "import tensorflow_hub as hub\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFTkpw1EbZ09"
      },
      "source": [
        "IMAGE_SIZE = 224\n",
        "model_name = 'efficientnet-lite0'\n",
        "model_handle = 'https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2'\n",
        "pixels = (224)\n",
        "\n",
        "print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "IMAGE_SHAPE = (pixels, pixels)\n",
        "print(f\"Input size {IMAGE_SIZE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyu1j3LOeq8C"
      },
      "source": [
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 64 \n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    width_shift_range=0.2,\n",
        "    brightness_range=[0.2,1.0]\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\",\n",
        ")\n",
        "\n",
        "\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        ")\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "    batch_size=(BATCH_SIZE),\n",
        "    class_mode='categorical',\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "    batch_size=(BATCH_SIZE),\n",
        "    class_mode='categorical',\n",
        ") \n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "    batch_size=(BATCH_SIZE),\n",
        "    class_mode='categorical'\n",
        ") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX0S0PQ0eQIk"
      },
      "source": [
        "classes = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Pwk03xdyyH"
      },
      "source": [
        "print(\"Building model with\", model_handle)\n",
        "model = tf.keras.Sequential([\n",
        "    # Explicitly define the input shape so the model can be properly\n",
        "    # loaded by the TFLiteConverter\n",
        "    tf.keras.layers.InputLayer(input_shape=IMAGE_SHAPE + (3,)),\n",
        "    hub.KerasLayer(model_handle, trainable=False),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(classes,\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SHAPE+(3,))\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfxSdtFka2Yx"
      },
      "source": [
        "checkpoint_path = \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\"                             \n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_path, verbose=1, save_weights_only=True,\n",
        "        period=1)\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2, restore_best_weights=True)\n",
        "\n",
        "rlrop = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience= 1, factor= 0.5, min_lr= 1e-15, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg4S8c0hdZvu"
      },
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.0005, momentum=0.9), \n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5, \n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stop, rlrop,cp_callback],).history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS6VeWAUeut9"
      },
      "source": [
        "def plot_hist(hist):\n",
        "    plt.suptitle('Loss and Accuracy Plots', fontsize=18)\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(hist['loss'], label='Training Loss')\n",
        "    plt.plot(hist['val_loss'], label='Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Number of epochs', fontsize=15)\n",
        "    plt.ylabel('Loss', fontsize=15)\n",
        "\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(hist['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(hist['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Number of epochs', fontsize=14)\n",
        "    plt.ylabel('Accuracy', fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "plot_hist(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt9RgB3wjR0L"
      },
      "source": [
        "def representative_data_gen():\n",
        "  dataset_list = tf.data.Dataset.list_files(park_dir + '/*/*')\n",
        "  for i in range(100):\n",
        "    image = next(iter(dataset_list))\n",
        "    image = tf.io.read_file(image)\n",
        "    image = tf.io.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "    image = tf.cast(image / 255., tf.float32)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    yield [image]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# This sets the representative dataset for quantization\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "# These set the input and output tensors to uint8 (added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('effnetlite_v2.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaQU8H21e4u0"
      },
      "source": [
        "batch_images, batch_labels = next(val_generator)\n",
        "\n",
        "logits = model(batch_images)\n",
        "prediction = np.argmax(logits, axis=1)\n",
        "truth = np.argmax(batch_labels, axis=1)\n",
        "\n",
        "keras_accuracy = tf.keras.metrics.Accuracy()\n",
        "keras_accuracy(prediction, truth)\n",
        "\n",
        "print(\"Raw model accuracy: {:.3%}\".format(keras_accuracy.result()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy4Mpa_Jjpwn"
      },
      "source": [
        "def set_input_tensor(interpreter, input):\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  tensor_index = input_details['index']\n",
        "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
        "  # Inputs for the TFLite model must be uint8, so we quantize our input data.\n",
        "  # NOTE: This step is necessary only because we're receiving input data from\n",
        "  # ImageDataGenerator, which rescaled all image data to float [0,1]. When using\n",
        "  # bitmap inputs, they're already uint8 [0,255] so this can be replaced with:\n",
        "  input_tensor[:, :] = input\n",
        "  scale, zero_point = input_details['quantization']\n",
        "  input_tensor[:, :] = np.uint8(input / scale + zero_point)\n",
        "\n",
        "def classify_image(interpreter, input,):\n",
        "  set_input_tensor(interpreter, input)\n",
        "  interpreter.invoke()\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "  output = interpreter.get_tensor(output_details['index'])\n",
        "  # print(batch_truth[c])\n",
        "  # Outputs from the TFLite model are uint8, so we dequantize the results:\n",
        "  scale, zero_point = output_details['quantization']\n",
        "  output = scale * (output - zero_point)\n",
        "  top_1 = np.argmin(output) #np.argmax(output)\n",
        "  # print(top_1)\n",
        "  return top_1\n",
        "\n",
        "interpreter = tf.lite.Interpreter('effnetlite_v2.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Collect all inference predictions in a list\n",
        "batch_prediction = []\n",
        "batch_truth = np.argmax(batch_labels, axis=1)\n",
        "\n",
        "c=0\n",
        "for i in range(len(batch_images)):\n",
        "  prediction = classify_image(interpreter, batch_images[i])\n",
        "  batch_prediction.append(prediction)\n",
        "\n",
        "# Compare all predictions to the ground truth\n",
        "tflite_accuracy = tf.keras.metrics.Accuracy()\n",
        "tflite_accuracy(batch_prediction, batch_truth)\n",
        "print(\"Quant TF Lite accuracy: {:.3%}\".format(tflite_accuracy.result()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELtZvsDljtyh"
      },
      "source": [
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "\n",
        "! sudo apt-get update\n",
        "\n",
        "! sudo apt-get install edgetpu-compiler\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hznibFEmDys"
      },
      "source": [
        "! edgetpu_compiler effnetlite_v2.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvcsukNTmMY7"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('effnetlite_v2.tflite')\n",
        "files.download('effnetlite_v2_edgetpu.tflite')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hccOOoRIy-CU"
      },
      "source": [
        "print (train_generator.class_indices)\n",
        "\n",
        "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
        "\n",
        "with open('lot_labels.txt', 'w') as f:\n",
        "  f.write(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okVvGUb3zAdr"
      },
      "source": [
        "image_batch, label_batch = next(val_generator)\n",
        "image_batch.shape, label_batch.shape\n",
        "\n",
        "class_names = sorted(train_generator.class_indices.keys())\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images in image_batch:\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image_batch[i])\n",
        "    plt.title(class_names[label_batch[i][1].astype('uint8')])\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}